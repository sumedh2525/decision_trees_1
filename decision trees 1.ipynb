{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbbb313c-f957-49be-98ad-b409db1ae511",
   "metadata": {},
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434558b2-c4b9-45e4-aaed-6f77192ff5f3",
   "metadata": {},
   "source": [
    "Initialization: The algorithm begins by considering the entire dataset as a starting node.\n",
    "\n",
    "Feature Selection: It evaluates different features in the dataset to determine which one is the best to split the data. The \"best\" feature is selected based on criteria that aim to minimize impurity within each resulting subset. Common impurity measures include Gini impurity and entropy.\n",
    "\n",
    "Splitting: The selected feature is used to split the dataset into subsets. Each subset represents a branch from the current node to a child node in the tree. The data points in each subset share the same value for the selected feature.\n",
    "\n",
    "Recursive Process: Steps 2 and 3 are repeated recursively for each child node until a stopping criterion is met. The stopping criterion could be reaching a maximum tree depth, having a minimum number of samples in a node, or achieving a certain level of purity.\n",
    "\n",
    "Leaf Node Assignment: Once the recursive splitting process ends, each leaf node contains a subset of data with predominantly one class label. The majority class label in each leaf node becomes the predicted label for that region of feature space.\n",
    "\n",
    "Prediction: To make a prediction for a new data point, it follows the decision path from the root node down to a specific leaf node by evaluating the values of features at each decision node. The class label associated with the leaf node reached becomes the predicted label for the input data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2020f1-77fe-4f6e-8840-9a85d3d4e875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0bffcd-7db8-4340-9ca1-b928e5cbcb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "950e7302-b1fe-4c17-85b0-1e07a962908b",
   "metadata": {},
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ce9c9-d9c6-42ea-9d63-cb7248a36bd7",
   "metadata": {},
   "source": [
    "Impurity Measures:\n",
    "Decision trees aim to create splits that result in pure subsets. A pure subset contains data points of a single class. To quantify impurity, we use impurity measures like Gini impurity and entropy. Lower impurity values indicate purer subsets.\n",
    "\n",
    "Gini Impurity:\n",
    "Gini impurity measures the probability of a randomly chosen element being misclassified. For a node with classes {1, 2, ..., C}, where C is the number of classes, the Gini impurity (Gini index) is calculated as:\n",
    "\n",
    "Gini\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "−\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "Gini(p)=1−∑ \n",
    "i=1\n",
    "C\n",
    "​\n",
    " p \n",
    "i\n",
    "2\n",
    "​\n",
    " \n",
    "\n",
    "Here, \n",
    "�\n",
    "�\n",
    "p \n",
    "i\n",
    "​\n",
    "  is the proportion of class \n",
    "�\n",
    "i in the node.\n",
    "\n",
    "Entropy:\n",
    "Entropy measures the average amount of information needed to classify a randomly chosen element. The entropy of a node is calculated as:\n",
    "\n",
    "Entropy\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "−\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "log\n",
    "⁡\n",
    "2\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "Entropy(p)=−∑ \n",
    "i=1\n",
    "C\n",
    "​\n",
    " p \n",
    "i\n",
    "​\n",
    " log \n",
    "2\n",
    "​\n",
    " (p \n",
    "i\n",
    "​\n",
    " )\n",
    "\n",
    "Again, \n",
    "�\n",
    "�\n",
    "p \n",
    "i\n",
    "​\n",
    "  is the proportion of class \n",
    "�\n",
    "i in the node.\n",
    "\n",
    "Splitting Criteria:\n",
    "The algorithm evaluates different features and their possible splits to find the best way to partition the data. The goal is to minimize impurity in the resulting subsets.\n",
    "\n",
    "Feature Selection:\n",
    "The algorithm chooses the feature that results in the most significant reduction in impurity. This is determined using metrics like the Gini gain or the Information Gain (reduction in entropy).\n",
    "\n",
    "For a dataset \n",
    "�\n",
    "D, the impurity of the initial node is \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "I(D), and after the split, we have subsets \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "D \n",
    "1\n",
    "​\n",
    " ,D \n",
    "2\n",
    "​\n",
    " ,…,D \n",
    "k\n",
    "​\n",
    " . The Gini gain (or Information Gain) is calculated as:\n",
    "\n",
    "Gain\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "−\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "∣\n",
    "�\n",
    "∣\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "Gain(D,A)=I(D)−∑ \n",
    "i=1\n",
    "k\n",
    "​\n",
    "  \n",
    "∣D∣\n",
    "∣D \n",
    "i\n",
    "​\n",
    " ∣\n",
    "​\n",
    " ⋅I(D \n",
    "i\n",
    "​\n",
    " )\n",
    "\n",
    "Here, \n",
    "�\n",
    "A represents the feature to split on.\n",
    "\n",
    "Recursive Splitting:\n",
    "The algorithm recursively applies the splitting process to the resulting subsets from the previous step. It continues this process until a stopping criterion is met, such as reaching a maximum tree depth or having a minimum number of samples in a node.\n",
    "\n",
    "Leaf Node Assignment:\n",
    "The tree structure is built with decision nodes representing feature tests and leaf nodes representing predicted classes. At each leaf node, the majority class in the subset becomes the predicted class.\n",
    "\n",
    "Prediction:\n",
    "To classify a new data point, it traverses down the tree from the root, following the decision rules based on the feature values. When it reaches a leaf node, the predicted class is the majority class in that leaf node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54015a6-7042-4885-82be-2eb1eaa484b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ed039-fc38-4cd3-919d-846b02964008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74895b94-ac67-47fb-b86f-9c5c17750b34",
   "metadata": {},
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664c371-cecf-4bdc-bdc0-5353e11c7fec",
   "metadata": {},
   "source": [
    "A Decision Tree Classifier can be used to solve a binary classification problem, where the goal is to classify data points into one of two classes (e.g., \"positive\" or \"negative,\" \"yes\" or \"no\"). Here's how the process works:\n",
    "\n",
    "Data Preparation:\n",
    "Prepare your dataset with features (input variables) and corresponding binary labels (0 or 1). Each data point should have a set of features and a corresponding binary label.\n",
    "\n",
    "Tree Construction:\n",
    "The decision tree algorithm builds a tree structure by recursively splitting the data based on the values of the features. It chooses the feature and split point that maximizes the reduction in impurity (e.g., Gini impurity or entropy).\n",
    "\n",
    "Splitting and Decision Nodes:\n",
    "At each decision node in the tree, a feature is selected, and a threshold value is chosen based on the data distribution. The data points are then split into two branches: one for which the feature value is less than or equal to the threshold, and another for which the feature value is greater than the threshold.\n",
    "\n",
    "Leaf Nodes and Class Assignment:\n",
    "The tree continues to split the data into subsets until certain stopping criteria are met (e.g., maximum depth or minimum number of samples in a node). At this point, the algorithm creates leaf nodes. The majority class of the data points in each leaf node becomes the predicted class for that node.\n",
    "\n",
    "Prediction:\n",
    "To classify a new data point, start from the root node of the tree and traverse the tree based on the values of the features. At each decision node, follow the appropriate branch based on whether the feature value is less than or greater than the threshold. Continue this process until you reach a leaf node. The predicted class for the new data point is the majority class of the training data in that leaf node.\n",
    "\n",
    "Model Evaluation:\n",
    "Once the decision tree is trained, you can evaluate its performance on a separate dataset or during cross-validation. Use metrics like accuracy, precision, recall, F1-score, or ROC-AUC to assess how well the model is performing on binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5612f7d-9df8-419d-bbc8-fe0cd2795986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4d768-ebc4-4c37-ae32-850307288fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c2b034e-2b1e-4c2b-8053-df9edf4803df",
   "metadata": {},
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240bea1-6f14-4b74-b30a-e8336cdc5fde",
   "metadata": {},
   "source": [
    "Feature Space Partitioning:\n",
    "Imagine you have a 2D dataset with two features, and you want to classify the data into two classes, say \"A\" and \"B.\" Each decision node in the tree corresponds to a condition on one of the features. For example, if the tree decides that feature 1 is greater than a threshold value, it might predict class \"A,\" and if the condition is not met, it might predict class \"B.\" This decision creates a boundary (hyperplane) that splits the feature space into two regions.\n",
    "\n",
    "Recursive Splitting:\n",
    "As the tree grows, it further partitions the feature space. Each internal node represents a decision boundary, and each leaf node represents a final class prediction. As you move down the tree, you're subdividing the feature space into smaller and smaller regions, with each region assigned a predicted class label.\n",
    "\n",
    "Decision Boundaries:\n",
    "The decision boundaries created by decision tree splits are orthogonal to the feature axes. This means that the boundary for each split is a straight line (or hyperplane) that is perpendicular to one of the features. This can be seen in the splits made by the tree as it moves down the tree structure.\n",
    "\n",
    "Prediction:\n",
    "To make predictions for new data points, you start at the root of the tree and follow the decision boundaries based on the values of the features. At each decision node, you decide whether to move left or right based on the condition defined by the split. This process continues until you reach a leaf node. The predicted class for the new data point is the majority class of the training data points that belong to the region associated with that leaf node.\n",
    "\n",
    "Visualization:\n",
    "Decision trees can be visualized to show the decision boundaries and the regions associated with different class predictions. In 2D, these boundaries are lines, and in higher dimensions, they become hyperplanes. By visualizing the tree, you can understand how it separates the feature space into different class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3ea7e-cd26-48b1-8298-8357ce2cf48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5accf7-de3b-4c76-ad07-9e90decca3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e78e0807-0eb0-42b0-b36b-74be23a0802e",
   "metadata": {},
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db2ee6-cb2a-441f-a25b-6bc97e17db40",
   "metadata": {},
   "source": [
    "True Positive (TP): The model correctly predicted instances as positive (class 1) that were actually positive in the ground truth.\n",
    "\n",
    "False Negative (FN): The model incorrectly predicted instances as negative (class 0) that were actually positive in the ground truth.\n",
    "\n",
    "False Positive (FP): The model incorrectly predicted instances as positive (class 1) that were actually negative in the ground truth.\n",
    "\n",
    "True Negative (TN): The model correctly predicted instances as negative (class 0) that were actually negative in the ground truth.\n",
    "\n",
    "The confusion matrix provides insights into the following evaluation metrics:\n",
    "\n",
    "Accuracy: The proportion of correctly predicted instances out of the total instances. It's calculated as:\n",
    "Accuracy\n",
    "\n",
    "Accuracy= (TP+TN) / (TP+TN+FP+FN)\n",
    "\n",
    "​\n",
    " \n",
    "\n",
    "Precision: Also known as positive predictive value, it's the proportion of true positive predictions out of all positive predictions. It measures the model's ability to avoid false positives. It's calculated as:\n",
    "Precision\n",
    "\n",
    "Precision= TP / (TP+FP)\n",
    " \n",
    "\n",
    "Recall: Also known as sensitivity or true positive rate, it's the proportion of true positive predictions out of all actual positive instances. It measures the model's ability to capture all positive instances. It's calculated as:\n",
    "Recall\n",
    "\n",
    "Recall= \n",
    "TP / (TP+FN)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall, providing a balanced measure of a model's performance. It's calculated as:\n",
    "F1-Score\n",
    "=\n",
    "\n",
    "F1-Score=   (2×Precision×Recall) / (Precision+Recall)\n",
    "\n",
    "\n",
    "Specificity: Also known as true negative rate, it's the proportion of true negative predictions out of all actual negative instances. It measures the model's ability to capture negative instances. It's calculated as:\n",
    "Specificity\n",
    "=\n",
    "\n",
    "Specificity=  TN / (TN+FP)\n",
    "\n",
    "\n",
    "False Positive Rate (FPR): The proportion of false positive predictions out of all actual negative instances. It's calculated as:\n",
    "FPR\n",
    "=\n",
    "\n",
    "FPR= FP / (TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433dab58-846a-43aa-9fb8-2d49533f4e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20623d3c-7f28-417b-bf44-3af7e7a1a019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e985505-f21c-4719-be7d-6cd0f3fcf243",
   "metadata": {},
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc742f9-f824-4658-acbc-7e4293a108b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8947368421052632\n",
      "Recall: 0.85\n",
      "F1-Score: 0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Confusion matrix values\n",
    "TP = 85\n",
    "FP = 10\n",
    "FN = 15\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a6a91-3e7c-46ec-becc-83f56a5f1d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea66eeb-d086-4f09-b016-ae6a6dd6cb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99d11c10-6fc1-49ad-9557-e93334a4ba31",
   "metadata": {},
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "explain how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bbe36-63db-49c5-889d-49f44951d8c2",
   "metadata": {},
   "source": [
    "Accuracy: This is the most straightforward metric, representing the ratio of correctly predicted instances to the total instances. However, accuracy can be misleading if the class distribution is imbalanced. For instance, in a dataset with 95% instances of Class A and 5% instances of Class B, a naive model that predicts Class A for all instances would still achieve 95% accuracy. Accuracy is suitable when classes are well-balanced and misclassifications of different classes have equal importance.\n",
    "\n",
    "Precision and Recall: Precision is the ratio of correctly predicted positive observations to the total predicted positives (true positives + false positives). Recall (also known as sensitivity or true positive rate) is the ratio of correctly predicted positive observations to the all actual positives (true positives + false negatives). Precision is important when the cost of false positives is high, while recall is crucial when the cost of false negatives is high. The F1-score, which is the harmonic mean of precision and recall, is useful for balancing the trade-off between these two metrics.\n",
    "\n",
    "Specificity and Negative Predictive Value: Specificity is the ratio of correctly predicted negative observations to the total actual negatives (true negatives + false positives). Negative Predictive Value is the ratio of correctly predicted negative observations to all predicted negatives (true negatives + false negatives). These metrics are especially relevant in medical and high-stakes applications where correctly identifying negatives is important.\n",
    "\n",
    "Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC): The ROC curve is a graphical representation of the true positive rate against the false positive rate at various classification thresholds. AUC quantifies the overall performance of a model, with higher values indicating better discrimination between classes. This metric is useful when assessing the model's performance across various threshold settings.\n",
    "\n",
    "Log Loss (Cross-Entropy): This metric measures the difference between predicted probabilities and actual outcomes. It is particularly useful when dealing with probabilistic models and can penalize models more heavily for confident incorrect predictions. Log loss aims to minimize the difference between predicted probabilities and actual outcomes.\n",
    "\n",
    "Matthews Correlation Coefficient (MCC): MCC takes into account all four confusion matrix values and is suitable for imbalanced datasets. It ranges from -1 (total disagreement) to +1 (perfect agreement) and considers both false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e49b71-1fc2-42f0-9a6d-ef6575f29396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64bf41-817a-43ff-8f6e-67bc0520e156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2da5029-ed33-4945-8bc4-e53b990d8407",
   "metadata": {},
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5b71f-19a1-457a-bfff-60b612515da4",
   "metadata": {},
   "source": [
    "Precision is the ratio of true positive predictions (correctly identified cases of the positive class) to all positive predictions (true positives + false positives). In medical diagnostics, the consequences of false positives can be significant and potentially harmful. False positives would mean that the model is incorrectly identifying individuals as having the disease when they don't actually have it. This could lead to unnecessary medical procedures, treatments, stress, and financial burden for the patients.\n",
    "\n",
    "In such a scenario, the emphasis is on minimizing false positives and ensuring that only truly positive cases are identified by the model. High precision means that when the model predicts a positive case, it is highly likely to be correct. This provides doctors and medical professionals with reliable information to make informed decisions.\n",
    "\n",
    "For instance, consider a test for a life-threatening disease where the treatment itself carries significant risks. If the model has high precision, doctors can be more confident that a positive prediction is indicative of the disease and proceed with appropriate interventions. On the other hand, if precision is low, there's a greater chance of false positives, which could lead to unnecessary treatments and undue stress for patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd233f3-cc1c-4b81-a077-f5eb79fbdaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec1d8f-9e66-428c-ac76-67c7981b4b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44289377-fda1-498a-8b2b-37fab5a9e0ea",
   "metadata": {},
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e6f0d-2dc2-417f-b7d2-0174d7c3f3be",
   "metadata": {},
   "source": [
    "Recall, also known as sensitivity or true positive rate, is the ratio of true positive predictions (correctly identified cases of the positive class) to all actual positive cases (true positives + false negatives). In the context of malware detection, the emphasis is on ensuring that all instances of malware are correctly identified, even if it means tolerating some false positives.\n",
    "\n",
    "Here's why recall is crucial in this scenario:\n",
    "\n",
    "Minimizing False Negatives: In cybersecurity, missing the detection of a malicious software (false negative) can have severe consequences. Malware can compromise systems, steal sensitive information, or cause widespread damage. If the model fails to detect even a single instance of malware, it could lead to a significant security breach.\n",
    "\n",
    "Immediate Action: When malware is detected, immediate action is often required to contain and mitigate its effects. A high recall rate ensures that potential threats are not missed, allowing cybersecurity teams to respond promptly and effectively.\n",
    "\n",
    "Tolerating False Positives: While false positives can be inconvenient, they are usually less harmful in this context compared to false negatives. False positives might trigger unnecessary alerts or actions, but they don't directly compromise the security of the system or data.\n",
    "\n",
    "Trade-off with Precision: While recall focuses on minimizing false negatives, it might result in more false positives. This trade-off is acceptable in cybersecurity because the primary concern is to catch all possible instances of malware, even if it means investigating some cases that turn out to be benign.\n",
    "\n",
    "In summary, in situations where the cost or impact of missing positive cases (false negatives) is high, as is the case in malware detection and other security-related tasks, recall becomes the most important evaluation metric. A high recall rate ensures that potential threats are identified, enabling timely and effective responses to maintain system security.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260dff18-11ed-496f-8adb-45bc016da2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8685f9-ddd7-4cd7-a48c-0610e9805ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
